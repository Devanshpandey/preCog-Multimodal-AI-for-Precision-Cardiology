{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4632cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "from PIL import Image  \n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from fastai.learner import Learner\n",
    "from fastai.data.core import DataLoaders\n",
    "from fastai.metrics import accuracy\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.callback.all import SaveModelCallback, EarlyStoppingCallback\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da54513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load datasets\n",
    "# df = pd.read_csv('Final_Datasets/liver.csv')\n",
    "# df_test = pd.read_csv('Final_Datasets/test_data_incidence.csv')\n",
    "# controls = pd.read_csv('Final_Datasets/imbalanced_control_iid.csv')\n",
    "\n",
    "# # Remove test cases and controls\n",
    "# test_iids = set(df_test['IID'])\n",
    "\n",
    "# # Get all cases (CAD = 1) from liver.csv, excluding test data\n",
    "# cases_train = df[(df['CAD'] == 1) & (~df['IID'].isin(test_iids))]\n",
    "\n",
    "# # Get all controls (IID in controls.csv and CAD = 0), excluding test data\n",
    "# controls_train = df[(df['IID'].isin(controls['IID'])) & (df['CAD'] == 0) & (~df['IID'].isin(test_iids))]\n",
    "\n",
    "# # Combine the imbalanced dataset\n",
    "# df_imbalanced_train = pd.concat([cases_train, controls_train]).reset_index(drop=True)\n",
    "\n",
    "# # Save the imbalanced dataset\n",
    "# df_imbalanced_train.to_csv('Final_Datasets/train_imbalanced_liver.csv', sep=',', index=False)\n",
    "\n",
    "# # Report the dataset statistics\n",
    "# cases_count = len(cases_train)\n",
    "# controls_count = len(controls_train)\n",
    "# print(f\"Number of cases: {cases_count}\")\n",
    "# print(f\"Number of controls: {controls_count}\")\n",
    "# print(f\"Imbalance ratio (controls to cases): {controls_count / cases_count:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b42bc760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_imbalanced_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ef7b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LiverMRI Dataset Class\n",
    "class LiverMRIDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_column_name, label_column_name, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_column_name = image_column_name\n",
    "        self.label_column_name = label_column_name\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe[self.image_column_name].iloc[idx]\n",
    "        label = self.dataframe[self.label_column_name].iloc[idx]\n",
    "\n",
    "        # Load the grayscale image\n",
    "        image = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "\n",
    "        # Convert grayscale to 3 channels\n",
    "        transform_to_3_channel = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3)\n",
    "        ])\n",
    "        image = transform_to_3_channel(image)\n",
    "\n",
    "        # Apply other transformations if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label, img_path\n",
    "\n",
    "# Backbone Class for Feature Extraction\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base_model = models.resnet50(pretrained=False)\n",
    "        encoder_layers = list(base_model.children())\n",
    "        self.backbone = nn.Sequential(*encoder_layers[:9])  # Use the first 9 layers of ResNet50\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Classifier Class (not used in embeddings extraction but included for completeness)\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.linear = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.drop_out(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Liver Embeddings Extraction Class\n",
    "class LiverDiseaseModelEmbeddings:\n",
    "    def __init__(self, train_df_path, test_df_path, image_column_name, label_column_name, batch_size=32, model_name='liver_mri_resnet50'):\n",
    "        self.train_df_path = train_df_path\n",
    "        self.test_df_path = test_df_path\n",
    "        self.image_column_name = image_column_name\n",
    "        self.label_column_name = label_column_name\n",
    "        self.batch_size = batch_size\n",
    "        self.model_name = model_name\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self._prepare_data()\n",
    "        self._prepare_model()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        train_df = pd.read_csv(self.train_df_path)\n",
    "        test_df = pd.read_csv(self.test_df_path)\n",
    "\n",
    "        # Liver MRI Dataset\n",
    "        self.train_dataset = LiverMRIDataset(train_df, self.image_column_name, self.label_column_name, transform=self._get_transforms())\n",
    "        self.test_dataset = LiverMRIDataset(test_df, self.image_column_name, self.label_column_name, transform=self._get_transforms())\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=8)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "    def _get_transforms(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def _prepare_model(self):\n",
    "        backbone = Backbone()\n",
    "        classifier = Classifier(num_classes=2)\n",
    "        model = nn.Sequential(backbone, classifier)\n",
    "        model.to(self.device)\n",
    "        self.model = model\n",
    "\n",
    "        # Load the fine-tuned model\n",
    "        self.model.load_state_dict(torch.load(f'{self.model_name}.pth'))\n",
    "        self.model.eval()\n",
    "\n",
    "    def extract_embeddings(self, loader):\n",
    "        embeddings, labels, paths = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for images, label_batch, path_batch in loader:  # Add path_batch\n",
    "                images = images.to(self.device)\n",
    "            \n",
    "                # Pass through the backbone only\n",
    "                x = self.model[0](images)  # Extract features from the Backbone\n",
    "                x = torch.flatten(x, 1)   # Flatten after global pooling\n",
    "            \n",
    "                embeddings.append(x.cpu().numpy())\n",
    "                labels.append(label_batch.cpu().numpy())\n",
    "                paths.extend(path_batch)  # Collect the image paths\n",
    "        embeddings = np.concatenate(embeddings)\n",
    "        labels = np.concatenate(labels)\n",
    "        return embeddings, labels, paths\n",
    "\n",
    "\n",
    "\n",
    "    # Updated generate_embeddings_dataframe function\n",
    "    def generate_embeddings_dataframe(self, embeddings, labels, paths):\n",
    "        \"\"\"\n",
    "        Creates a Pandas DataFrame from embeddings, labels, and image paths.\n",
    "\n",
    "        Args:\n",
    "            embeddings (numpy.ndarray): The extracted embeddings.\n",
    "            labels (numpy.ndarray): The labels corresponding to the embeddings.\n",
    "            paths (list of str): The image paths.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame with serialized embeddings and metadata.\n",
    "        \"\"\"\n",
    "        # Serialize embeddings as JSON strings for safe CSV storage\n",
    "        df = pd.DataFrame({\n",
    "            'image_path': paths,\n",
    "            'embedding': [json.dumps(emb.tolist()) for emb in embeddings],\n",
    "            'label': labels\n",
    "        })\n",
    "        return df\n",
    "\n",
    "     #Updated extract_and_save_embeddings function\n",
    "    def extract_and_save_embeddings(self):\n",
    "        \"\"\"\n",
    "        Extracts embeddings for train and test datasets and saves them as CSV files.\n",
    "\n",
    "        The embeddings are serialized as JSON strings for robust CSV storage.\n",
    "        \"\"\"\n",
    "        # Extract training embeddings\n",
    "        train_embeddings, train_labels, train_paths = self.extract_embeddings(self.train_loader)\n",
    "        train_df = self.generate_embeddings_dataframe(train_embeddings, train_labels, train_paths)\n",
    "\n",
    "        # Extract test embeddings\n",
    "        test_embeddings, test_labels, test_paths = self.extract_embeddings(self.test_loader)\n",
    "        test_df = self.generate_embeddings_dataframe(test_embeddings, test_labels, test_paths)\n",
    "\n",
    "        # Save DataFrames to CSV with JSON-serialized embeddings\n",
    "        train_df.to_csv('train_embeddings_liver_nov_imbalanced.csv', index=False)\n",
    "        test_df.to_csv('test_embeddings_liver_nov.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f790d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/07880/devansh/anaconda3/envs/pyt_env/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/work/07880/devansh/anaconda3/envs/pyt_env/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_4641/1449034207.py:96: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(f'{self.model_name}.pth'))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = LiverDiseaseModelEmbeddings(\n",
    "        train_df_path='Final_Datasets/train_imbalanced_liver.csv',  # Path to liver train dataset\n",
    "        test_df_path='Final_Datasets/test_data_incidence.csv',  # Path to liver test dataset\n",
    "        image_column_name='FilePath_liver',                    # Liver image file paths\n",
    "        label_column_name='CAD',                               # CAD labels\n",
    "        model_name='models/liver_mri_radIM_resnet50_model_nov'        # Fine-tuned liver model name\n",
    "    )\n",
    "    \n",
    "    # Extract embeddings and save to CSV\n",
    "    model.extract_and_save_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cada854d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>embedding</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/corral-repl/utexas/UKB-Imaging-Genetics/Imaging_Data/processed_imaging_data/MRI/Abdominal/field_20204_2_cropped/2283985.jpg</td>\n",
       "      <td>[0.048257745802402496, 0.17932827770709991, 0.2461884319782257, 0.18811623752117157, 0.05363137647509575, 0.22287796437740326, 0.16087187826633453, 0.19072483479976654, 0.12066276371479034, 0.24892660975456238, 0.021761098876595497, 0.29642313718795776, 0.165064737200737, 0.4340325593948364, 0.26540088653564453, 0.2743842303752899, 0.138432577252388, 0.3450947105884552, 0.21279335021972656, 0.16314095258712769, 0.1393556147813797, 0.1841920167207718, 0.22848498821258545, 0.14593391120433807, 0.45362401008605957, 0.10949797183275223, 0.20782503485679626, 0.3812257647514343, 0.33487343788146...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/corral-repl/utexas/UKB-Imaging-Genetics/Imaging_Data/processed_imaging_data/MRI/Abdominal/field_20204_2_cropped/1414804.jpg</td>\n",
       "      <td>[0.49649253487586975, 0.5446419715881348, 0.33405622839927673, 0.678919792175293, 0.8624733090400696, 0.3321053683757782, 0.382202684879303, 0.1793617308139801, 0.2632859945297241, 0.2910124361515045, 0.16421186923980713, 0.10440715402364731, 0.1475459635257721, 0.3331582844257355, 0.29979977011680603, 0.8486292958259583, 0.21641682088375092, 0.514647364616394, 0.4642202854156494, 0.6870560050010681, 0.24264007806777954, 0.4795040786266327, 0.05379239842295647, 0.10512242466211319, 0.3960431218147278, 0.19685517251491547, 0.5484703779220581, 0.4000292420387268, 0.11509321630001068, 0.44486...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/corral-repl/utexas/UKB-Imaging-Genetics/Imaging_Data/processed_imaging_data/MRI/Abdominal/field_20204_2_cropped/2008383.jpg</td>\n",
       "      <td>[0.4503157436847687, 0.25143322348594666, 0.3439211845397949, 0.5088186860084534, 0.3570999503135681, 0.3991461396217346, 0.33143702149391174, 0.2714579105377197, 0.25446099042892456, 0.5045296549797058, 0.370547890663147, 0.32849961519241333, 0.12461522221565247, 0.5313747525215149, 0.46944621205329895, 0.6066666841506958, 0.3636687994003296, 0.0677860826253891, 0.28542250394821167, 0.4170055687427521, 0.3903954029083252, 0.5325046181678772, 0.21283753216266632, 0.06155705824494362, 0.5240745544433594, 0.101762555539608, 0.35842669010162354, 0.4869183301925659, 0.06481748819351196, 0.2946...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/corral-repl/utexas/UKB-Imaging-Genetics/Imaging_Data/processed_imaging_data/MRI/Abdominal/field_20204_2_cropped/4557403.jpg</td>\n",
       "      <td>[0.454449325799942, 0.22551193833351135, 0.30657628178596497, 0.6026245355606079, 0.6241907477378845, 0.31453508138656616, 0.5085590481758118, 0.2247134894132614, 0.4495599567890167, 0.19871528446674347, 0.3039184510707855, 0.12345006316900253, 0.13531652092933655, 0.6168326139450073, 0.3546350300312042, 0.6485339999198914, 0.26137158274650574, 0.34863513708114624, 0.2726458013057709, 0.6379393339157104, 0.5042950510978699, 0.41701602935791016, 0.1594209372997284, 0.099574513733387, 0.3822794556617737, 0.18017825484275818, 0.4591946303844452, 0.32769888639450073, 0.16499248147010803, 0.139...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/corral-repl/utexas/UKB-Imaging-Genetics/Imaging_Data/processed_imaging_data/MRI/Abdominal/field_20204_2_cropped/5959204.jpg</td>\n",
       "      <td>[0.2629716396331787, 0.30035993456840515, 0.20724470913410187, 0.49216428399086, 0.6814802289009094, 0.0695125088095665, 0.16787882149219513, 0.08988457173109055, 0.1653607338666916, 0.21452203392982483, 0.12987765669822693, 0.16204451024532318, 0.17137178778648376, 0.4457346498966217, 0.14675098657608032, 0.50738126039505, 0.3554674983024597, 0.29122301936149597, 0.22758160531520844, 0.5015839338302612, 0.323143869638443, 0.20330536365509033, 0.04844409599900246, 0.045219000428915024, 0.25817713141441345, 0.23309996724128723, 0.4273673892021179, 0.25063008069992065, 0.30794137716293335, 0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                     image_path  \\\n",
       "0  /corral-repl/utexas/UKB-Imaging-Genetics/Imaging_Data/processed_imaging_data/MRI/Abdominal/field_20204_2_cropped/2283985.jpg   \n",
       "1  /corral-repl/utexas/UKB-Imaging-Genetics/Imaging_Data/processed_imaging_data/MRI/Abdominal/field_20204_2_cropped/1414804.jpg   \n",
       "2  /corral-repl/utexas/UKB-Imaging-Genetics/Imaging_Data/processed_imaging_data/MRI/Abdominal/field_20204_2_cropped/2008383.jpg   \n",
       "3  /corral-repl/utexas/UKB-Imaging-Genetics/Imaging_Data/processed_imaging_data/MRI/Abdominal/field_20204_2_cropped/4557403.jpg   \n",
       "4  /corral-repl/utexas/UKB-Imaging-Genetics/Imaging_Data/processed_imaging_data/MRI/Abdominal/field_20204_2_cropped/5959204.jpg   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 embedding  \\\n",
       "0  [0.048257745802402496, 0.17932827770709991, 0.2461884319782257, 0.18811623752117157, 0.05363137647509575, 0.22287796437740326, 0.16087187826633453, 0.19072483479976654, 0.12066276371479034, 0.24892660975456238, 0.021761098876595497, 0.29642313718795776, 0.165064737200737, 0.4340325593948364, 0.26540088653564453, 0.2743842303752899, 0.138432577252388, 0.3450947105884552, 0.21279335021972656, 0.16314095258712769, 0.1393556147813797, 0.1841920167207718, 0.22848498821258545, 0.14593391120433807, 0.45362401008605957, 0.10949797183275223, 0.20782503485679626, 0.3812257647514343, 0.33487343788146...   \n",
       "1  [0.49649253487586975, 0.5446419715881348, 0.33405622839927673, 0.678919792175293, 0.8624733090400696, 0.3321053683757782, 0.382202684879303, 0.1793617308139801, 0.2632859945297241, 0.2910124361515045, 0.16421186923980713, 0.10440715402364731, 0.1475459635257721, 0.3331582844257355, 0.29979977011680603, 0.8486292958259583, 0.21641682088375092, 0.514647364616394, 0.4642202854156494, 0.6870560050010681, 0.24264007806777954, 0.4795040786266327, 0.05379239842295647, 0.10512242466211319, 0.3960431218147278, 0.19685517251491547, 0.5484703779220581, 0.4000292420387268, 0.11509321630001068, 0.44486...   \n",
       "2  [0.4503157436847687, 0.25143322348594666, 0.3439211845397949, 0.5088186860084534, 0.3570999503135681, 0.3991461396217346, 0.33143702149391174, 0.2714579105377197, 0.25446099042892456, 0.5045296549797058, 0.370547890663147, 0.32849961519241333, 0.12461522221565247, 0.5313747525215149, 0.46944621205329895, 0.6066666841506958, 0.3636687994003296, 0.0677860826253891, 0.28542250394821167, 0.4170055687427521, 0.3903954029083252, 0.5325046181678772, 0.21283753216266632, 0.06155705824494362, 0.5240745544433594, 0.101762555539608, 0.35842669010162354, 0.4869183301925659, 0.06481748819351196, 0.2946...   \n",
       "3  [0.454449325799942, 0.22551193833351135, 0.30657628178596497, 0.6026245355606079, 0.6241907477378845, 0.31453508138656616, 0.5085590481758118, 0.2247134894132614, 0.4495599567890167, 0.19871528446674347, 0.3039184510707855, 0.12345006316900253, 0.13531652092933655, 0.6168326139450073, 0.3546350300312042, 0.6485339999198914, 0.26137158274650574, 0.34863513708114624, 0.2726458013057709, 0.6379393339157104, 0.5042950510978699, 0.41701602935791016, 0.1594209372997284, 0.099574513733387, 0.3822794556617737, 0.18017825484275818, 0.4591946303844452, 0.32769888639450073, 0.16499248147010803, 0.139...   \n",
       "4  [0.2629716396331787, 0.30035993456840515, 0.20724470913410187, 0.49216428399086, 0.6814802289009094, 0.0695125088095665, 0.16787882149219513, 0.08988457173109055, 0.1653607338666916, 0.21452203392982483, 0.12987765669822693, 0.16204451024532318, 0.17137178778648376, 0.4457346498966217, 0.14675098657608032, 0.50738126039505, 0.3554674983024597, 0.29122301936149597, 0.22758160531520844, 0.5015839338302612, 0.323143869638443, 0.20330536365509033, 0.04844409599900246, 0.045219000428915024, 0.25817713141441345, 0.23309996724128723, 0.4273673892021179, 0.25063008069992065, 0.30794137716293335, 0...   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = pd.read_csv('train_embeddings_liver_nov_imbalanced.csv')\n",
    "emb['embedding'] = emb['embedding'].apply(lambda x: np.array(json.loads(x)))\n",
    "emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64f75d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb['embedding'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc041280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    26358\n",
       "1     4342\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848b6376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_env",
   "language": "python",
   "name": "pyt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
