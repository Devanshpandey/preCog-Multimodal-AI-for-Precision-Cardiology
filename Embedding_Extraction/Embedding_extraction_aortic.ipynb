{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce62cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "from PIL import Image  \n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from monai.transforms import (\n",
    "    Compose, RandFlip, RandRotate, RandAffine, Resize\n",
    ")\n",
    "import torch.nn as nn\n",
    "from fastai.learner import Learner\n",
    "from monai.networks.nets import resnet18\n",
    "from fastai.data.core import DataLoaders\n",
    "from fastai.metrics import accuracy\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.callback.all import SaveModelCallback, EarlyStoppingCallback\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7849ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load datasets\n",
    "# df = pd.read_csv('Final_Datasets/blood_flow_heart.csv')\n",
    "# df_test = pd.read_csv('Final_Datasets/test_data_incidence.csv')\n",
    "# controls = pd.read_csv('Final_Datasets/imbalanced_control_iid.csv')\n",
    "\n",
    "# # Remove test cases and controls\n",
    "# test_iids = set(df_test['IID'])\n",
    "\n",
    "# # Get all cases (CAD = 1) from blood_flow_heart.csv, excluding test data\n",
    "# cases_train = df[(df['CAD'] == 1) & (~df['IID'].isin(test_iids))]\n",
    "\n",
    "# # Get all controls (IID in controls.csv and CAD = 0), excluding test data\n",
    "# controls_train = df[(df['IID'].isin(controls['IID'])) & (df['CAD'] == 0) & (~df['IID'].isin(test_iids))]\n",
    "\n",
    "# # Combine the imbalanced dataset\n",
    "# df_imbalanced_train = pd.concat([cases_train, controls_train]).reset_index(drop=True)\n",
    "\n",
    "# # Save the imbalanced dataset\n",
    "# df_imbalanced_train.to_csv('Final_Datasets/train_imbalanced_blood_flow.csv', sep=',', index=False)\n",
    "\n",
    "# # Report the dataset statistics\n",
    "# cases_count = len(cases_train)\n",
    "# controls_count = len(controls_train)\n",
    "# print(f\"Number of cases: {cases_count}\")\n",
    "# print(f\"Number of controls: {controls_count}\")\n",
    "# print(f\"Imbalance ratio (controls to cases): {controls_count / cases_count:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5af15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NpyDataset(Dataset):\n",
    "#     def __init__(self, dataframe, image_column_name, label_column_name):\n",
    "#         self.dataframe = dataframe.reset_index(drop=True)\n",
    "#         self.image_column_name = image_column_name\n",
    "#         self.label_column_name = label_column_name\n",
    "#         self.transform = transforms.Compose([\n",
    "#             transforms.Lambda(lambda x: torch.stack([x, x, x], dim=0) if x.size(0) == 18 else x),  # Stack to create 3 channels\n",
    "#             transforms.Resize((112, 112))  # Resize to 112x112\n",
    "#         ])\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataframe)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         npy_path = self.dataframe.loc[idx, self.image_column_name]\n",
    "#         label = self.dataframe.loc[idx, self.label_column_name]\n",
    "#         image = np.load(npy_path)  # Load NPY file (e.g., shape: (30, 200, 200))\n",
    "\n",
    "#         # Access channel and frames specific to blood flow data \n",
    "#         image = image[6:24, :, :]  # Selecting frames 6 to 23 (to get 18 frames)\n",
    "\n",
    "#         image = torch.tensor(image, dtype=torch.float32)  # Convert to tensor\n",
    "#         image = self.transform(image)  # Apply transforms\n",
    "#         label = torch.tensor(label, dtype=torch.long)  # Convert label to tensor\n",
    "#         return {'image': image, 'label': label, 'path': npy_path}\n",
    "    \n",
    "\n",
    "\n",
    "# class HeartDiseaseModelBloodFlow:\n",
    "#     def __init__(self, train_df_path, test_df_path, image_column_name, label_column_name, batch_size=32, model_name='blood_flow_model'):\n",
    "#         self.train_df_path = train_df_path\n",
    "#         self.test_df_path = test_df_path\n",
    "#         self.image_column_name = image_column_name\n",
    "#         self.label_column_name = label_column_name\n",
    "#         self.batch_size = batch_size\n",
    "#         self.model_name = model_name\n",
    "#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "#         self._prepare_data()\n",
    "#         self._prepare_model()\n",
    "\n",
    "#     def _prepare_data(self):\n",
    "#         train_df = pd.read_csv(self.train_df_path)\n",
    "#         test_df = pd.read_csv(self.test_df_path)\n",
    "\n",
    "#         self.train_dataset = NpyDataset(train_df, self.image_column_name, self.label_column_name)\n",
    "#         self.test_dataset = NpyDataset(test_df, self.image_column_name, self.label_column_name)\n",
    "\n",
    "#         self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=8)\n",
    "#         self.test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "#     def _prepare_model(self):\n",
    "#         self.model = models.video.r3d_18(pretrained=True)\n",
    "#         self.model.fc = nn.Linear(self.model.fc.in_features, 2)\n",
    "#         self.model.to(self.device)\n",
    "\n",
    "#         self.learn = Learner(\n",
    "#             DataLoaders(self.train_loader, self.test_loader),\n",
    "#             self.model,\n",
    "#             loss_func=CrossEntropyLossFlat(),\n",
    "#             metrics=[accuracy],\n",
    "#             wd=1e-4,\n",
    "#             cbs=[\n",
    "#                 SaveModelCallback(fname=self.model_name, monitor='train_loss'),\n",
    "#                 EarlyStoppingCallback(monitor='train_loss', patience=10)\n",
    "#             ]\n",
    "#         ).to_fp16()\n",
    "\n",
    "#     def train(self, epochs=5, lr=1e-5):\n",
    "#         self.learn.fine_tune(epochs, base_lr=lr)\n",
    "\n",
    "#     def extract_embeddings(self, loader):\n",
    "#         self.model.eval()\n",
    "#         embeddings, labels, paths = [], [], []\n",
    "#         with torch.no_grad():\n",
    "#             for batch in loader:\n",
    "#                 images = batch['image']\n",
    "#                 label_batch = batch['label']\n",
    "#                 path_batch = batch['path']\n",
    "#                 images = images.to(self.device)\n",
    "                \n",
    "#                 # Pass through the model until the final pooling layer\n",
    "#                 x = self.model.stem(images)\n",
    "#                 x = self.model.layer1(x)\n",
    "#                 x = self.model.layer2(x)\n",
    "#                 x = self.model.layer3(x)\n",
    "#                 x = self.model.layer4(x)\n",
    "#                 x = self.model.avgpool(x)\n",
    "#                 x = torch.flatten(x, 1)\n",
    "                \n",
    "#                 embeddings.append(x.cpu().numpy())\n",
    "#                 labels.append(label_batch.cpu().numpy())\n",
    "#                 paths.extend(path_batch)  # Collecting the paths\n",
    "#         embeddings = np.concatenate(embeddings)\n",
    "#         labels = np.concatenate(labels)\n",
    "#         return embeddings, labels, paths\n",
    "\n",
    "#     # Updated generate_embeddings_dataframe function\n",
    "#     def generate_embeddings_dataframe(self, embeddings, labels, paths):\n",
    "#         \"\"\"\n",
    "#         Creates a Pandas DataFrame from embeddings, labels, and image paths.\n",
    "\n",
    "#         Args:\n",
    "#             embeddings (numpy.ndarray): The extracted embeddings.\n",
    "#             labels (numpy.ndarray): The labels corresponding to the embeddings.\n",
    "#             paths (list of str): The image paths.\n",
    "\n",
    "#         Returns:\n",
    "#             pd.DataFrame: A DataFrame with serialized embeddings and metadata.\n",
    "#         \"\"\"\n",
    "#         # Serialize embeddings as JSON strings for safe CSV storage\n",
    "#         df = pd.DataFrame({\n",
    "#             'image_path': paths,\n",
    "#             'embedding': [json.dumps(emb.tolist()) for emb in embeddings],\n",
    "#             'label': labels\n",
    "#         })\n",
    "#         return df\n",
    "\n",
    "#     # Updated extract_and_save_embeddings function\n",
    "#     def extract_and_save_embeddings(self):\n",
    "#         \"\"\"\n",
    "#         Extracts embeddings for train and test datasets and saves them as CSV files.\n",
    "\n",
    "#         The embeddings are serialized as JSON strings for robust CSV storage.\n",
    "#         \"\"\"\n",
    "#         # Extract training embeddings\n",
    "#         train_embeddings, train_labels, train_paths = self.extract_embeddings(self.train_loader)\n",
    "#         train_df = self.generate_embeddings_dataframe(train_embeddings, train_labels, train_paths)\n",
    "\n",
    "#         # Extract test embeddings\n",
    "#         test_embeddings, test_labels, test_paths = self.extract_embeddings(self.test_loader)\n",
    "#         test_df = self.generate_embeddings_dataframe(test_embeddings, test_labels, test_paths)\n",
    "        \n",
    "#         # Save DataFrames\n",
    "#         train_df.to_csv('train_embeddings_blood_flow_nov_imbalanced.csv', index=False)\n",
    "#         test_df.to_csv('test_embeddings_blood_flow_nov.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9672d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     model = HeartDiseaseModelBloodFlow(\n",
    "#         train_df_path='Final_Datasets/train_imbalanced_blood_flow.csv',  # Updated for blood flow dataset\n",
    "#         test_df_path='Final_Datasets/test_data_incidence.csv',      # Updated for test data\n",
    "#         image_column_name='FilePath_bf',  # Replace with blood flow image path column name\n",
    "#         label_column_name='CAD',          # Replace with blood flow label column name\n",
    "#         model_name='heart_blood_flow'\n",
    "#     )\n",
    "#     # Uncomment the following line if you need to train the model first\n",
    "#     # model.train(epochs=50, lr=1e-5)\n",
    "    \n",
    "#     # Extract embeddings and save to CSV\n",
    "#     model.extract_and_save_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6518f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb = pd.read_csv('train_embeddings_blood_flow_nov_imbalanced.csv')\n",
    "# emb['embedding'] = emb['embedding'].apply(lambda x: np.array(json.loads(x)))\n",
    "# emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ded68f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(emb['embedding'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817381ce",
   "metadata": {},
   "source": [
    "**MONAI RESNET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e456abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NPYDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Custom PyTorch Dataset for loading 3D medical imaging data from .npy files.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, dataframe, image_column_name, label_column_name):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             dataframe (pd.DataFrame): The dataframe containing the data.\n",
    "#             image_column_name (str): Name of the column containing the image file paths.\n",
    "#             label_column_name (str): Name of the column containing the labels.\n",
    "#         \"\"\"\n",
    "#         self.dataframe = dataframe.reset_index(drop=True)\n",
    "#         self.image_column_name = image_column_name\n",
    "#         self.label_column_name = label_column_name\n",
    "\n",
    "#         # Transformation pipeline\n",
    "#         self.transform = transforms.Compose([\n",
    "#             transforms.Resize((112, 112))  # Resize to 112x112\n",
    "#         ])\n",
    "\n",
    "#     def __len__(self):\n",
    "#         \"\"\"Return the number of samples in the dataset.\"\"\"\n",
    "#         return len(self.dataframe)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"\n",
    "#         Load and preprocess a single sample.\n",
    "\n",
    "#         Args:\n",
    "#             idx (int): Index of the sample to fetch.\n",
    "\n",
    "#         Returns:\n",
    "#             dict: Contains the processed image, label, and original file path.\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             npy_path = self.dataframe.loc[idx, self.image_column_name]\n",
    "#             label = self.dataframe.loc[idx, self.label_column_name]\n",
    "\n",
    "#             # Load the 3D array (e.g., shape: [30, 200, 200])\n",
    "#             image = np.load(npy_path)\n",
    "\n",
    "#             # Select exactly 18 frames for the model\n",
    "#             image = image[6:24, :, :]  # Shape: [18, 200, 200]\n",
    "\n",
    "#             # Convert to tensor and add channel dimension\n",
    "#             image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)  # Shape: [1, 18, 200, 200]\n",
    "\n",
    "#             # Resize spatial dimensions (height and width) to 112x112\n",
    "#             image = F.interpolate(image, size=(112, 112), mode='bilinear', align_corners=False)\n",
    "\n",
    "#             # Apply transformations\n",
    "#             image = self.transform(image)\n",
    "\n",
    "#             label = torch.tensor(label, dtype=torch.long)  # Convert label to tensor\n",
    "\n",
    "#             return {'image': image, 'label': label, 'path': npy_path}\n",
    "\n",
    "#         except Exception as e:\n",
    "#             raise RuntimeError(f\"Error processing file at index {idx}: {e}\")\n",
    "            \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "from PIL import Image  \n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from monai.transforms import (\n",
    "    Compose, Resize\n",
    ")\n",
    "import torch.nn as nn\n",
    "from fastai.learner import Learner\n",
    "from monai.networks.nets import resnet18\n",
    "from fastai.data.core import DataLoaders\n",
    "from fastai.metrics import accuracy\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.callback.all import SaveModelCallback, EarlyStoppingCallback\n",
    "\n",
    "class NPYDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for loading 3D medical imaging data from .npy files.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, image_column_name, label_column_name):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_column_name = image_column_name\n",
    "        self.label_column_name = label_column_name\n",
    "\n",
    "        # Transformation pipeline\n",
    "        self.transform = Compose([\n",
    "            Resize(spatial_size=(18, 112, 112))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and preprocess a single sample.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            npy_path = self.dataframe[self.image_column_name].iloc[idx]\n",
    "            label = self.dataframe[self.label_column_name].iloc[idx]\n",
    "\n",
    "            # Load the 3D array (100 frames, 224x224 pixels)\n",
    "            image = np.load(npy_path)  # Shape: (100, 224, 224)\n",
    "\n",
    "            # Select exactly 18 frames\n",
    "            image = image[45:63, :, :]  # Shape: (18, 224, 224)\n",
    "\n",
    "            # Convert to tensor and add channel dimension\n",
    "            image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)  # Shape: (1, 18, 224, 224)\n",
    "\n",
    "            # Resize spatial dimensions to 112x112\n",
    "            image = F.interpolate(image, size=(112, 112), mode='bilinear', align_corners=False)\n",
    "\n",
    "            # Apply transformations\n",
    "            image = self.transform(image)\n",
    "\n",
    "            return {'image': image, 'label': label, 'path': npy_path}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image at index {idx}: {e}\")\n",
    "            raise\n",
    "\n",
    "class HeartDiseaseModelBloodFlow:\n",
    "    def __init__(self, train_df_path, test_df_path, image_column_name, label_column_name, batch_size=32, model_name='heart_ch0_3channel_MONAI_resnet18'):\n",
    "        self.train_df_path = train_df_path\n",
    "        self.test_df_path = test_df_path\n",
    "        self.image_column_name = image_column_name\n",
    "        self.label_column_name = label_column_name\n",
    "        self.batch_size = batch_size\n",
    "        self.model_name = model_name\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self._prepare_data()\n",
    "        self._load_model()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        train_df = pd.read_csv(self.train_df_path)\n",
    "        test_df = pd.read_csv(self.test_df_path)\n",
    "\n",
    "        self.train_dataset = NPYDataset(train_df, self.image_column_name, self.label_column_name)\n",
    "        self.test_dataset = NPYDataset(test_df, self.image_column_name, self.label_column_name)\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=8)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "    def _load_model(self):\n",
    "        # Create an instance of MONAI's ResNet18\n",
    "        self.model = resnet18(spatial_dims=3, n_input_channels=1, num_classes=2)\n",
    "\n",
    "        # Initialize FastAI Learner\n",
    "        self.learn = Learner(\n",
    "            DataLoaders(self.train_loader, self.test_loader),\n",
    "            self.model,\n",
    "            loss_func=CrossEntropyLossFlat(),\n",
    "            metrics=[accuracy]\n",
    "        ).to_fp16()\n",
    "\n",
    "        # Load the fine-tuned model\n",
    "        self.learn.load(self.model_name)\n",
    "        self.model = self.learn.model.eval().to(self.device)\n",
    "\n",
    "    ### Extract Embeddings (Separate)\n",
    "    def extract_embeddings(self, loader):\n",
    "        embeddings, labels, paths = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                images = batch['image'].to(self.device)\n",
    "                label_batch = batch['label']\n",
    "                path_batch = batch['path']\n",
    "                \n",
    "                # Extract embeddings from the penultimate layer\n",
    "                x = self.model.conv1(images)\n",
    "                x = self.model.bn1(x)\n",
    "                x = self.model.maxpool(x)\n",
    "                x = self.model.layer1(x)\n",
    "                x = self.model.layer2(x)\n",
    "                x = self.model.layer3(x)\n",
    "                x = self.model.layer4(x)\n",
    "                x = self.model.avgpool(x)\n",
    "                x = torch.flatten(x, 1)\n",
    "\n",
    "                embeddings.append(x.cpu().numpy())\n",
    "                labels.append(label_batch.cpu().numpy())\n",
    "                paths.extend(path_batch)  \n",
    "                \n",
    "        embeddings = np.concatenate(embeddings)\n",
    "        labels = np.concatenate(labels)\n",
    "        return embeddings, labels, paths\n",
    "\n",
    "    def generate_embeddings_dataframe(self, embeddings, labels, paths):\n",
    "        df = pd.DataFrame({\n",
    "            'image_path': paths,\n",
    "            'embedding': [json.dumps(emb.tolist()) for emb in embeddings],\n",
    "            'label': labels\n",
    "        })\n",
    "        return df\n",
    "\n",
    "    def extract_and_save_embeddings(self):\n",
    "        train_embeddings, train_labels, train_paths = self.extract_embeddings(self.train_loader)\n",
    "        train_df = self.generate_embeddings_dataframe(train_embeddings, train_labels, train_paths)\n",
    "\n",
    "        test_embeddings, test_labels, test_paths = self.extract_embeddings(self.test_loader)\n",
    "        test_df = self.generate_embeddings_dataframe(test_embeddings, test_labels, test_paths)\n",
    "\n",
    "        train_df.to_csv('train_embeddings_aortic_monai.csv', index=False)\n",
    "        test_df.to_csv('test_embeddings_aortic_monai.csv', index=False)\n",
    "\n",
    "    ### Extract Predicted Probabilities (Separate)\n",
    "    def extract_probabilities(self, loader):\n",
    "        probabilities, labels, paths = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                images = batch['image'].to(self.device)\n",
    "                label_batch = batch['label']\n",
    "                path_batch = batch['path']\n",
    "\n",
    "                logits = self.model(images)  # Direct model output\n",
    "                probs = F.softmax(logits, dim=1)[:, 1]  # Probability of CAD = 1\n",
    "\n",
    "                probabilities.append(probs.cpu().numpy())\n",
    "                labels.append(label_batch.cpu().numpy())\n",
    "                paths.extend(path_batch)\n",
    "            \n",
    "        probabilities = np.concatenate(probabilities)\n",
    "        labels = np.concatenate(labels)\n",
    "        return probabilities, labels, paths\n",
    "\n",
    "    def generate_probabilities_dataframe(self, probabilities, labels, paths):\n",
    "        df = pd.DataFrame({\n",
    "            'image_path': paths,\n",
    "            'predicted_probability': probabilities,\n",
    "            'label': labels\n",
    "        })\n",
    "        return df\n",
    "\n",
    "    def extract_and_save_probabilities(self):\n",
    "        train_probs, train_labels, train_paths = self.extract_probabilities(self.train_loader)\n",
    "        train_df = self.generate_probabilities_dataframe(train_probs, train_labels, train_paths)\n",
    "\n",
    "        test_probs, test_labels, test_paths = self.extract_probabilities(self.test_loader)\n",
    "        test_df = self.generate_probabilities_dataframe(test_probs, test_labels, test_paths)\n",
    "\n",
    "        train_df.to_csv('train_aortic_predicted_probabilities.csv', index=False)\n",
    "        test_df.to_csv('test_aortic_predicted_probabilities.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d953f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = HeartDiseaseModelBloodFlow(\n",
    "        train_df_path='Final_Datasets/train_resnet_ao.csv',\n",
    "        test_df_path='Final_Datasets/test_incidence_new.csv',\n",
    "        image_column_name='FilePath_ao',\n",
    "        label_column_name='CAD',\n",
    "        model_name='heart_ao_Med3D_resnet18'\n",
    "    )\n",
    "    \n",
    "    model.extract_and_save_embeddings()  # Save embeddings\n",
    "    model.extract_and_save_probabilities()  # Save predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547dac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_embeddings_aortic_monai.csv')\n",
    "train['embedding'] = train['embedding'].apply(lambda x: np.array(json.loads(x)))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726cc65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label to categorical (optional for better plotting)\n",
    "train['label'] = train['label'].astype(str)  # Ensure labels are treated as categorical\n",
    "\n",
    "# Create a box plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=train['label'], y=train['predicted_probability'], palette=\"Set2\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"True Label (CAD)\", fontsize=12)\n",
    "plt.ylabel(\"Predicted Probability (P(CAD=1))\", fontsize=12)\n",
    "plt.title(\"Predicted Probabilities by True Label\", fontsize=14)\n",
    "\n",
    "# Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9cb65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_embeddings_aortic_monai.csv')\n",
    "test['embedding'] = test['embedding'].apply(lambda x: np.array(json.loads(x)))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b649a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming train and test dataframes are named 'train' and 'test'\n",
    "# Expand embeddings into separate columns for train and test\n",
    "embedding_columns = [f'embedding_{i}' for i in range(len(train['embedding'][0]))]\n",
    "train_embedding_df = pd.DataFrame(train['embedding'].tolist(), columns=embedding_columns)\n",
    "train_data = pd.concat([train_embedding_df, train['label']], axis=1)\n",
    "\n",
    "test_embedding_df = pd.DataFrame(test['embedding'].tolist(), columns=embedding_columns)\n",
    "test_data = pd.concat([test_embedding_df, test['label']], axis=1)\n",
    "\n",
    "# Balance the training data\n",
    "cases = train_data[train_data['label'] == 1]\n",
    "controls = train_data[train_data['label'] == 0]\n",
    "min_size = min(len(cases), len(controls))\n",
    "cases_balanced = cases.sample(n=min_size, random_state=42)\n",
    "controls_balanced = controls.sample(n=min_size, random_state=42)\n",
    "balanced_train_data = pd.concat([cases_balanced, controls_balanced])\n",
    "\n",
    "# Define features and target\n",
    "X_train = balanced_train_data[embedding_columns]\n",
    "y_train = balanced_train_data['label']\n",
    "X_test = test_data[embedding_columns]\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Number of PCs to use\n",
    "pcs_to_use = [5, 10, 15, 20, 25, len(embedding_columns)]\n",
    "# Dictionary to store ROC data and accuracy\n",
    "roc_data = {}\n",
    "\n",
    "# Train models and evaluate\n",
    "for num_pcs in pcs_to_use:\n",
    "    if num_pcs == len(embedding_columns):\n",
    "        X_train_selected = X_train\n",
    "        X_test_selected = X_test\n",
    "    else:\n",
    "        X_train_selected = X_train_pca[:, :num_pcs]\n",
    "        X_test_selected = X_test_pca[:, :num_pcs]\n",
    "    \n",
    "    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test_selected)\n",
    "    y_pred_proba = model.predict_proba(X_test_selected)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    \n",
    "    # Store data\n",
    "    roc_data[num_pcs] = (fpr, tpr, auc, accuracy)\n",
    "\n",
    "# Plot the ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "for num_pcs, (fpr, tpr, auc, accuracy) in roc_data.items():\n",
    "    label = (\n",
    "        f\"{num_pcs} PCs (AUC = {auc:.2f}, Acc = {accuracy:.2f})\"\n",
    "        if num_pcs != len(embedding_columns)\n",
    "        else f\"All Embeddings (AUC = {auc:.2f}, Acc = {accuracy:.2f})\"\n",
    "    )\n",
    "    plt.plot(fpr, tpr, label=label)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.title(\"ROC Curves for Different Number of PCs for Aortic Distensibilty\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f1a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(\"train_embeddings_aortic_monai.csv\")\n",
    "test = pd.read_csv(\"test_embeddings_aortic_monai.csv\")\n",
    "\n",
    "# Convert JSON-encoded embeddings into NumPy arrays\n",
    "train[\"embedding\"] = train[\"embedding\"].apply(lambda x: np.array(json.loads(x)))\n",
    "test[\"embedding\"] = test[\"embedding\"].apply(lambda x: np.array(json.loads(x)))\n",
    "\n",
    "# Extract embeddings and labels\n",
    "X_train = np.vstack(train[\"embedding\"].values)\n",
    "X_test = np.vstack(test[\"embedding\"].values)\n",
    "y_train = train[\"label\"].values\n",
    "y_test = test[\"label\"].values\n",
    "\n",
    "# Apply PCA to embeddings\n",
    "pca = PCA(n_components=25)  # Keep 25 PCs\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Number of principal components to evaluate (including full embedding case)\n",
    "pcs_to_use = [5, 10, 15, 20, 25]  # Already PCA transformed\n",
    "pcs_to_use.append(\"Full Embedding\")  # Full embeddings without PCA reduction\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store results\n",
    "cv_results = []\n",
    "test_results = []\n",
    "\n",
    "# Train models and evaluate\n",
    "for num_pcs in pcs_to_use:\n",
    "    if num_pcs == \"Full Embedding\":\n",
    "        X_train_selected = X_train  # Use full raw embeddings\n",
    "        X_test_selected = X_test\n",
    "    else:\n",
    "        X_train_selected = X_train_pca[:, :num_pcs]\n",
    "        X_test_selected = X_test_pca[:, :num_pcs]\n",
    "\n",
    "    # Store all cross-validation performance for box plots\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_selected, y_train)):\n",
    "        X_train_fold, X_val_fold = X_train_selected[train_idx], X_train_selected[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        # Train XGBoost classifier\n",
    "        model = XGBClassifier(eval_metric=\"logloss\")\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Predict and evaluate on validation set\n",
    "        y_val_pred_proba = model.predict_proba(X_val_fold)[:, 1]\n",
    "        auc_val = roc_auc_score(y_val_fold, y_val_pred_proba)\n",
    "        acc_val = accuracy_score(y_val_fold, (y_val_pred_proba > 0.5).astype(int))\n",
    "\n",
    "        # Store all CV results\n",
    "        cv_results.append({\n",
    "            \"Num_PCs\": num_pcs,\n",
    "            \"Fold\": fold + 1,\n",
    "            \"AUC\": auc_val,\n",
    "            \"Accuracy\": acc_val\n",
    "        })\n",
    "\n",
    "    # Train on full training set and evaluate on test set\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    y_test_pred_proba = model.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "    auc_test = roc_auc_score(y_test, y_test_pred_proba)\n",
    "    acc_test = accuracy_score(y_test, (y_test_pred_proba > 0.5).astype(int))\n",
    "\n",
    "    # Store test results\n",
    "    test_results.append({\n",
    "        \"Num_PCs\": num_pcs,\n",
    "        \"AUC\": auc_test,\n",
    "        \"Accuracy\": acc_test\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrames and save as CSV\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "test_results_df = pd.DataFrame(test_results)\n",
    "\n",
    "cv_results_csv_path = \"pca_xgboost_cv_results.csv\"\n",
    "test_results_csv_path = \"pca_xgboost_test_results.csv\"\n",
    "\n",
    "cv_results_df.to_csv(cv_results_csv_path, index=False)\n",
    "test_results_df.to_csv(test_results_csv_path, index=False)\n",
    "\n",
    "print(f\"Cross-validation results saved to {cv_results_csv_path}\")\n",
    "print(f\"Test results saved to {test_results_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d348218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30749"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d3ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_env",
   "language": "python",
   "name": "pyt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
