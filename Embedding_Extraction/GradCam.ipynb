{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3edfd2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!/usr/bin/env python\n",
    "# import os\n",
    "# import logging\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import transforms\n",
    "# from monai.networks.nets import resnet18  # MONAI’s 3D ResNet18\n",
    "# from fastai.learner import Learner\n",
    "# from fastai.data.core import DataLoaders\n",
    "# from fastai.metrics import accuracy\n",
    "# from fastai.losses import CrossEntropyLossFlat\n",
    "# from fastai.callback.all import SaveModelCallback, EarlyStoppingCallback\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.backends.backend_pdf import PdfPages\n",
    "# import seaborn as sns\n",
    "\n",
    "# # =============================================================================\n",
    "# # Configure Logging\n",
    "# # =============================================================================\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO,\n",
    "#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "#     handlers=[\n",
    "#         logging.FileHandler(\"heart_disease_model.log\"),\n",
    "#         logging.StreamHandler()\n",
    "#     ]\n",
    "# )\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # =============================================================================\n",
    "# # Data Loading: Custom Dataset for .npy Files\n",
    "# # =============================================================================\n",
    "# class NPYDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Custom PyTorch Dataset for loading 3D medical imaging data from .npy files.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, dataframe, image_column_name, label_column_name, custom_transform=None):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             dataframe (pd.DataFrame): DataFrame with file paths and labels.\n",
    "#             image_column_name (str): Column name for image file paths.\n",
    "#             label_column_name (str): Column name for labels.\n",
    "#             custom_transform (callable, optional): Custom transform pipeline.\n",
    "#         \"\"\"\n",
    "#         self.dataframe = dataframe\n",
    "#         self.image_column_name = image_column_name\n",
    "#         self.label_column_name = label_column_name\n",
    "\n",
    "#         # Default transform (applied to the tensor)\n",
    "#         default_transform = transforms.Compose([\n",
    "#             transforms.RandomHorizontalFlip(p=0.5),\n",
    "#             transforms.RandomRotation(15),\n",
    "#             transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "#             transforms.Resize((112, 112)),  # Med3D default size\n",
    "#         ])\n",
    "#         self.transform = custom_transform if custom_transform is not None else default_transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataframe)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         try:\n",
    "#             npy_path = self.dataframe[self.image_column_name].iloc[idx]\n",
    "#             label = self.dataframe[self.label_column_name].iloc[idx]\n",
    "#             # Load image: assuming shape [D, H, W, Channels]. We take the first channel.\n",
    "#             image = np.load(npy_path)[:, :, :, 0]\n",
    "#             image = image[17:33, :, :]  # Select frames 17 to 32\n",
    "#             # Convert to tensor and add a channel dimension: (1, D, H, W)\n",
    "#             image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)\n",
    "#             # Apply transforms\n",
    "#             image = self.transform(image)\n",
    "#             return image, label\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Error loading image at index {idx}: {e}\")\n",
    "#             raise\n",
    "\n",
    "# # =============================================================================\n",
    "# # Model Definition (Only Loading – No Training)\n",
    "# # =============================================================================\n",
    "# class HeartDiseaseModel:\n",
    "#     \"\"\"\n",
    "#     Class to load data and the model. (Training is skipped; we only load saved weights.)\n",
    "#     \"\"\"\n",
    "#     def __init__(self, config):\n",
    "#         self.config = self._validate_config(config)\n",
    "#         self.logger = logging.getLogger(self.__class__.__name__)\n",
    "#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         self.logger.info(f\"Using device: {self.device}\")\n",
    "#         self._prepare_data()\n",
    "#         self._prepare_model()\n",
    "\n",
    "#     def _validate_config(self, config):\n",
    "#         default_config = {\n",
    "#             'train_dataframe_path': None,\n",
    "#             'test_dataframe_path': None,\n",
    "#             'image_column_name': 'FilePath',\n",
    "#             'label_column_name': 'CAD',\n",
    "#             'pretrained_weights_path': None,  # initial weights file (used to initialize the model)\n",
    "#             'batch_size': 8,\n",
    "#             'split_ratio': 0.85,\n",
    "#             'model_name': 'heart_ch0_3channel_MONAI_resnet18',  # name used by fastai to save the model\n",
    "#             'learning_rate': 1e-5,\n",
    "#             'epochs': 50,\n",
    "#             'early_stopping_patience': 20,\n",
    "#             'weight_decay': 1e-4\n",
    "#         }\n",
    "#         default_config.update(config)\n",
    "#         required_paths = [\n",
    "#             'train_dataframe_path',\n",
    "#             'test_dataframe_path',\n",
    "#             'pretrained_weights_path'\n",
    "#         ]\n",
    "#         for key in required_paths:\n",
    "#             if not default_config[key] or not os.path.exists(default_config[key]):\n",
    "#                 raise ValueError(f\"Invalid or missing path for {key}: {default_config[key]}\")\n",
    "#         return default_config\n",
    "\n",
    "#     def _prepare_data(self):\n",
    "#         try:\n",
    "#             train_df = pd.read_csv(self.config['train_dataframe_path'])\n",
    "#             test_df = pd.read_csv(self.config['test_dataframe_path'])\n",
    "#             # Create dataset from training CSV\n",
    "#             dataset = NPYDataset(\n",
    "#                 train_df,\n",
    "#                 self.config['image_column_name'],\n",
    "#                 self.config['label_column_name']\n",
    "#             )\n",
    "#             # Split training data into training and validation subsets\n",
    "#             train_size = int(self.config['split_ratio'] * len(dataset))\n",
    "#             val_size = len(dataset) - train_size\n",
    "#             self.train_dataset, self.val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "#             self.train_loader = DataLoader(\n",
    "#                 self.train_dataset,\n",
    "#                 batch_size=self.config['batch_size'],\n",
    "#                 shuffle=True,\n",
    "#                 num_workers=8\n",
    "#             )\n",
    "#             self.val_loader = DataLoader(\n",
    "#                 self.val_dataset,\n",
    "#                 batch_size=self.config['batch_size'],\n",
    "#                 shuffle=False,\n",
    "#                 num_workers=8\n",
    "#             )\n",
    "#             # Prepare test dataset\n",
    "#             self.test_dataset = NPYDataset(\n",
    "#                 test_df,\n",
    "#                 self.config['image_column_name'],\n",
    "#                 self.config['label_column_name']\n",
    "#             )\n",
    "#             self.test_loader = DataLoader(\n",
    "#                 self.test_dataset,\n",
    "#                 batch_size=self.config['batch_size'],\n",
    "#                 shuffle=False,\n",
    "#                 num_workers=8\n",
    "#             )\n",
    "#             self.logger.info(\"Data preparation completed successfully.\")\n",
    "#         except Exception as e:\n",
    "#             self.logger.error(f\"Error in data preparation: {e}\")\n",
    "#             raise\n",
    "\n",
    "#     def _prepare_model(self):\n",
    "#         try:\n",
    "#             # Initialize the MONAI ResNet18 model for 3D data.\n",
    "#             self.model = resnet18(\n",
    "#                 spatial_dims=3,\n",
    "#                 n_input_channels=1,   # assuming grayscale input\n",
    "#                 num_classes=2         # binary classification (e.g., CAD vs. No CAD)\n",
    "#             )\n",
    "#             self.logger.info(\"Loading initial pretrained weights...\")\n",
    "#             state_dict = torch.load(self.config['pretrained_weights_path'], map_location=self.device)\n",
    "#             self.model.load_state_dict(state_dict, strict=False)\n",
    "#             # Wrap in DataParallel and send to device.\n",
    "#             self.model = nn.DataParallel(self.model)\n",
    "#             self.model.to(self.device)\n",
    "#             # Create FastAI DataLoaders and Learner.\n",
    "#             self.dls = DataLoaders(self.train_loader, self.val_loader)\n",
    "#             self.learn = Learner(\n",
    "#                 self.dls,\n",
    "#                 self.model,\n",
    "#                 loss_func=CrossEntropyLossFlat(),\n",
    "#                 metrics=[accuracy],\n",
    "#                 wd=self.config['weight_decay'],\n",
    "#                 cbs=[\n",
    "#                     SaveModelCallback(\n",
    "#                         fname=self.config['model_name'],\n",
    "#                         monitor='valid_loss'\n",
    "#                     ),\n",
    "#                     EarlyStoppingCallback(\n",
    "#                         monitor='valid_loss',\n",
    "#                         patience=self.config['early_stopping_patience']\n",
    "#                     )\n",
    "#                 ]\n",
    "#             ).to_fp16()\n",
    "#             self.logger.info(\"Model preparation completed successfully.\")\n",
    "#         except Exception as e:\n",
    "#             self.logger.error(f\"Error in model preparation: {e}\")\n",
    "#             raise\n",
    "\n",
    "# # =============================================================================\n",
    "# # GradCAM Implementation for 3D Data\n",
    "# # =============================================================================\n",
    "# class GradCam3D:\n",
    "#     \"\"\"\n",
    "#     A simple Grad-CAM implementation for 3D models.\n",
    "#     Registers forward and full-backward hooks on a target convolutional layer.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, model, target_layer):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             model (torch.nn.Module): The underlying model (not the DataParallel wrapper).\n",
    "#             target_layer (torch.nn.Module): The layer on which to register hooks.\n",
    "#         \"\"\"\n",
    "#         self.model = model\n",
    "#         self.target_layer = target_layer\n",
    "#         self.gradients = None\n",
    "#         self.activations = None\n",
    "#         self.hook_handles = []\n",
    "#         self._register_hooks()\n",
    "\n",
    "#     def _register_hooks(self):\n",
    "#         def forward_hook(module, input, output):\n",
    "#             self.activations = output.detach()\n",
    "\n",
    "#         def backward_hook(module, grad_input, grad_output):\n",
    "#             self.gradients = grad_output[0].detach()\n",
    "\n",
    "#         self.hook_handles.append(self.target_layer.register_forward_hook(forward_hook))\n",
    "#         self.hook_handles.append(self.target_layer.register_full_backward_hook(backward_hook))\n",
    "\n",
    "#     def remove_hooks(self):\n",
    "#         for handle in self.hook_handles:\n",
    "#             handle.remove()\n",
    "\n",
    "#     def __call__(self, input_tensor, target_class=None):\n",
    "#         \"\"\"\n",
    "#         Generates the CAM for the input.\n",
    "        \n",
    "#         Args:\n",
    "#             input_tensor (torch.Tensor): Input tensor of shape (B, C, D, H, W).\n",
    "#             target_class (torch.Tensor or None): If provided, these class indices will be used.\n",
    "        \n",
    "#         Returns:\n",
    "#             torch.Tensor: CAMs with shape (B, D, H, W) normalized between 0 and 1.\n",
    "#         \"\"\"\n",
    "#         # Forward pass.\n",
    "#         output = self.model(input_tensor)\n",
    "#         if target_class is None:\n",
    "#             target_class = output.argmax(dim=1)\n",
    "#         one_hot = torch.zeros_like(output)\n",
    "#         for i, tc in enumerate(target_class):\n",
    "#             one_hot[i, tc] = 1\n",
    "\n",
    "#         self.model.zero_grad()\n",
    "#         output.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "#         # Retrieve stored activations and gradients.\n",
    "#         gradients = self.gradients  # shape: (B, C, d, h, w)\n",
    "#         activations = self.activations  # shape: (B, C, d, h, w)\n",
    "#         # Global average pooling over depth, height, and width.\n",
    "#         weights = torch.mean(gradients, dim=(2, 3, 4), keepdim=True)  # shape: (B, C, 1, 1, 1)\n",
    "#         cam = torch.sum(weights * activations, dim=1)  # shape: (B, d, h, w)\n",
    "#         cam = F.relu(cam)  # Apply ReLU\n",
    "\n",
    "#         # Normalize each CAM individually.\n",
    "#         cams = []\n",
    "#         for i in range(cam.shape[0]):\n",
    "#             cam_i = cam[i]\n",
    "#             cam_i = cam_i - cam_i.min()\n",
    "#             if cam_i.max() != 0:\n",
    "#                 cam_i = cam_i / cam_i.max()\n",
    "#             cams.append(cam_i)\n",
    "#         cams = torch.stack(cams)  # shape: (B, d, h, w)\n",
    "#         # Upsample the CAM to match the input resolution.\n",
    "#         target_size = input_tensor.shape[2:]  # (D, H, W)\n",
    "#         cams = F.interpolate(cams.unsqueeze(1), size=target_size, mode='trilinear', align_corners=False)\n",
    "#         cams = cams.squeeze(1)  # shape: (B, D, H, W)\n",
    "#         return cams\n",
    "\n",
    "# # =============================================================================\n",
    "# # Advanced Visualization: Montage of Multiple Slices\n",
    "# # =============================================================================\n",
    "# def create_montage_for_sample(model, gradcam, sample, n_slices=5):\n",
    "#     \"\"\"\n",
    "#     Creates a montage (grid) of multiple slices from a sample.\n",
    "    \n",
    "#     For each of n evenly spaced slices in the 3D volume, this function plots:\n",
    "#       - The original image slice.\n",
    "#       - The same slice overlaid with the CAM.\n",
    "    \n",
    "#     Args:\n",
    "#         model (torch.nn.Module): The underlying trained model.\n",
    "#         gradcam (GradCam3D): An instance of GradCam3D.\n",
    "#         sample (torch.Tensor): Image tensor of shape (C, D, H, W).\n",
    "#         n_slices (int): Number of slices to display.\n",
    "        \n",
    "#     Returns:\n",
    "#         fig (matplotlib.figure.Figure): The figure containing the montage.\n",
    "#     \"\"\"\n",
    "#     device = next(model.parameters()).device\n",
    "#     input_tensor = sample.unsqueeze(0).to(device)\n",
    "#     cams = gradcam(input_tensor)  # shape: (1, D, H, W)\n",
    "#     cam = cams[0].cpu().numpy()\n",
    "#     original = sample.squeeze(0).cpu().numpy()  # shape: (D, H, W)\n",
    "#     D = original.shape[0]\n",
    "#     indices = np.linspace(0, D-1, n_slices, dtype=int)\n",
    "    \n",
    "#     fig, axs = plt.subplots(n_slices, 2, figsize=(8, 2.5 * n_slices))\n",
    "#     for i, idx in enumerate(indices):\n",
    "#         axs[i, 0].imshow(original[idx], cmap='gray')\n",
    "#         axs[i, 0].set_title(f\"Slice {idx} Original\")\n",
    "#         axs[i, 0].axis('off')\n",
    "        \n",
    "#         axs[i, 1].imshow(original[idx], cmap='gray')\n",
    "#         axs[i, 1].imshow(cam[idx], cmap='jet', alpha=0.5)\n",
    "#         axs[i, 1].set_title(f\"Slice {idx} CAM Overlay\")\n",
    "#         axs[i, 1].axis('off')\n",
    "#     fig.tight_layout()\n",
    "#     return fig\n",
    "\n",
    "# def create_combined_montage(model, gradcam, sample_control, sample_case, n_slices=5, title=\"Comparison\"):\n",
    "#     \"\"\"\n",
    "#     Creates a side-by-side montage comparing control and case samples.\n",
    "    \n",
    "#     For each sample, n slices are shown; for each slice, the original and CAM overlay are\n",
    "#     displayed. The resulting figure has 4 columns:\n",
    "#       - Column 1: Control Original.\n",
    "#       - Column 2: Control CAM Overlay.\n",
    "#       - Column 3: Case Original.\n",
    "#       - Column 4: Case CAM Overlay.\n",
    "    \n",
    "#     Args:\n",
    "#         model (torch.nn.Module): The underlying trained model.\n",
    "#         gradcam (GradCam3D): Instance of GradCam3D.\n",
    "#         sample_control (tuple): (image, label) for the control sample.\n",
    "#         sample_case (tuple): (image, label) for the case sample.\n",
    "#         n_slices (int): Number of slices to display.\n",
    "#         title (str): Title for the figure.\n",
    "    \n",
    "#     Returns:\n",
    "#         fig (matplotlib.figure.Figure): The combined figure.\n",
    "#     \"\"\"\n",
    "#     device = next(model.parameters()).device\n",
    "\n",
    "#     # Get CAM and original for control.\n",
    "#     input_control = sample_control[0].unsqueeze(0).to(device)\n",
    "#     cams_control = gradcam(input_control)[0].cpu().numpy()\n",
    "#     orig_control = sample_control[0].squeeze(0).cpu().numpy()\n",
    "\n",
    "#     # Get CAM and original for case.\n",
    "#     input_case = sample_case[0].unsqueeze(0).to(device)\n",
    "#     cams_case = gradcam(input_case)[0].cpu().numpy()\n",
    "#     orig_case = sample_case[0].squeeze(0).cpu().numpy()\n",
    "\n",
    "#     D = orig_control.shape[0]\n",
    "#     indices = np.linspace(0, D-1, n_slices, dtype=int)\n",
    "\n",
    "#     fig, axs = plt.subplots(n_slices, 4, figsize=(16, 2.5 * n_slices))\n",
    "#     fig.suptitle(title, fontsize=16)\n",
    "#     for i, idx in enumerate(indices):\n",
    "#         # Control Original.\n",
    "#         axs[i, 0].imshow(orig_control[idx], cmap='gray')\n",
    "#         axs[i, 0].set_title(f\"Control Slice {idx}\\nOriginal\", fontsize=10)\n",
    "#         axs[i, 0].axis('off')\n",
    "#         # Control CAM.\n",
    "#         axs[i, 1].imshow(orig_control[idx], cmap='gray')\n",
    "#         axs[i, 1].imshow(cams_control[idx], cmap='jet', alpha=0.5)\n",
    "#         axs[i, 1].set_title(f\"Control Slice {idx}\\nCAM\", fontsize=10)\n",
    "#         axs[i, 1].axis('off')\n",
    "#         # Case Original.\n",
    "#         axs[i, 2].imshow(orig_case[idx], cmap='gray')\n",
    "#         axs[i, 2].set_title(f\"Case Slice {idx}\\nOriginal\", fontsize=10)\n",
    "#         axs[i, 2].axis('off')\n",
    "#         # Case CAM.\n",
    "#         axs[i, 3].imshow(orig_case[idx], cmap='gray')\n",
    "#         axs[i, 3].imshow(cams_case[idx], cmap='jet', alpha=0.5)\n",
    "#         axs[i, 3].set_title(f\"Case Slice {idx}\\nCAM\", fontsize=10)\n",
    "#         axs[i, 3].axis('off')\n",
    "#     fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "#     return fig\n",
    "\n",
    "# # =============================================================================\n",
    "# # Utility to Get a Sample by Label from a Dataset\n",
    "# # =============================================================================\n",
    "# def get_sample_by_label(dataset, target_label):\n",
    "#     \"\"\"\n",
    "#     Returns the first sample from the dataset with the specified label.\n",
    "    \n",
    "#     Args:\n",
    "#         dataset (torch.utils.data.Dataset): The dataset to search.\n",
    "#         target_label (int): The label to search for.\n",
    "    \n",
    "#     Returns:\n",
    "#         tuple: (image, label) or None if not found.\n",
    "#     \"\"\"\n",
    "#     for i in range(len(dataset)):\n",
    "#         try:\n",
    "#             image, label = dataset[i]\n",
    "#             if label == target_label:\n",
    "#                 return image, label\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Error accessing sample {i}: {e}\")\n",
    "#     return None\n",
    "\n",
    "# # =============================================================================\n",
    "# # Main Execution Function (Load Trained Model and Generate Advanced Visualizations)\n",
    "# # =============================================================================\n",
    "# def main():\n",
    "#     try:\n",
    "#         # Update these paths for your environment.\n",
    "#         config = {\n",
    "#             'train_dataframe_path': 'Final_Datasets/train_resnet_heart.csv',\n",
    "#             'test_dataframe_path': 'Final_Datasets/test_data_incidence.csv',\n",
    "#             'pretrained_weights_path': '../Med3D/resnet_18_23dataset.pth',  # initial weights file path\n",
    "#             'model_name': 'heart_ch0_3channel_MONAI_resnet18',  # name used by fastai's SaveModelCallback\n",
    "#             'epochs': 50,\n",
    "#             'learning_rate': 1e-5,\n",
    "#             'batch_size': 8,\n",
    "#             'split_ratio': 0.85,\n",
    "#             'early_stopping_patience': 20,\n",
    "#             'weight_decay': 1e-4\n",
    "#         }\n",
    "\n",
    "#         # Instantiate the model and load data (no training is performed).\n",
    "#         heart_disease_model = HeartDiseaseModel(config)\n",
    "\n",
    "#         # Load the trained model weights saved by FastAI.\n",
    "#         heart_disease_model.learn.load(config['model_name'])\n",
    "#         logger.info(\"Loaded trained model weights.\")\n",
    "\n",
    "#         # Retrieve the underlying model (if wrapped in DataParallel, use .module).\n",
    "#         trained_model = heart_disease_model.learn.model\n",
    "#         if isinstance(trained_model, torch.nn.DataParallel):\n",
    "#             base_model = trained_model.module\n",
    "#         else:\n",
    "#             base_model = trained_model\n",
    "\n",
    "#         # Select a target layer for Grad-CAM.\n",
    "#         target_layer = base_model.layer3[1].conv2\n",
    "\n",
    "#         # Initialize GradCAM with the underlying module.\n",
    "#         gradcam = GradCam3D(base_model, target_layer)\n",
    "\n",
    "#         # Get one control and one case sample from training data.\n",
    "#         train_control = get_sample_by_label(heart_disease_model.train_dataset, target_label=0)\n",
    "#         train_case = get_sample_by_label(heart_disease_model.train_dataset, target_label=1)\n",
    "\n",
    "#         # Get one control and one case sample from test data.\n",
    "#         test_control = get_sample_by_label(heart_disease_model.test_dataset, target_label=0)\n",
    "#         test_case = get_sample_by_label(heart_disease_model.test_dataset, target_label=1)\n",
    "\n",
    "#         pdf_filename = \"advanced_gradcam_comparison.pdf\"\n",
    "#         with PdfPages(pdf_filename) as pdf:\n",
    "#             # Advanced montage for Training Data.\n",
    "#             if train_control is not None and train_case is not None:\n",
    "#                 fig_train = create_combined_montage(base_model, gradcam, train_control, train_case,\n",
    "#                                                     n_slices=5, title=\"Training Data Comparison: Control vs. Case\")\n",
    "#                 pdf.savefig(fig_train)\n",
    "#                 plt.close(fig_train)\n",
    "#             else:\n",
    "#                 print(\"Missing training samples for comparison.\")\n",
    "\n",
    "#             # Advanced montage for Test Data.\n",
    "#             if test_control is not None and test_case is not None:\n",
    "#                 fig_test = create_combined_montage(base_model, gradcam, test_control, test_case,\n",
    "#                                                    n_slices=5, title=\"Test Data Comparison: Control vs. Case\")\n",
    "#                 pdf.savefig(fig_test)\n",
    "#                 plt.close(fig_test)\n",
    "#             else:\n",
    "#                 print(\"Missing test samples for comparison.\")\n",
    "#         print(f\"Saved advanced comparison figures to {pdf_filename}\")\n",
    "\n",
    "#         # Optionally, you can also save individual montages:\n",
    "#         # For example:\n",
    "#         # fig_montage = create_montage_for_sample(base_model, gradcam, train_control, n_slices=7)\n",
    "#         # fig_montage.savefig(\"train_control_montage.pdf\")\n",
    "#         # plt.close(fig_montage)\n",
    "\n",
    "#         # Remove hooks when finished.\n",
    "#         gradcam.remove_hooks()\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Critical error in main execution: {e}\")\n",
    "#         raise\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cf001a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2615676/3093307609.py:104: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(config['pretrained_weights_path'], map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Grad-CAM results to GradCAM_Results.pdf\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from monai.networks.nets import resnet18  # MONAI’s 3D ResNet18\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# =============================================================================\n",
    "# Configure Logging\n",
    "# =============================================================================\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# =============================================================================\n",
    "# Data Loading: Custom Dataset for .npy Files\n",
    "# =============================================================================\n",
    "class NPYDataset(Dataset):\n",
    "    \"\"\" Custom PyTorch Dataset for loading 3D medical imaging data from .npy files. \"\"\"\n",
    "    def __init__(self, dataframe, image_column_name, label_column_name):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_column_name = image_column_name\n",
    "        self.label_column_name = label_column_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            npy_path = self.dataframe[self.image_column_name].iloc[idx]\n",
    "            label = self.dataframe[self.label_column_name].iloc[idx]\n",
    "            image = np.load(npy_path)[:, :, :, 0]  # Load single-channel data\n",
    "            image = image[17:33, :, :]  # Select slices 17 to 32\n",
    "            image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)  # Shape (1, D, H, W)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading image at index {idx}: {e}\")\n",
    "            raise\n",
    "\n",
    "# =============================================================================\n",
    "# Grad-CAM Implementation for 3D Data\n",
    "# =============================================================================\n",
    "class GradCam3D:\n",
    "    \"\"\" Grad-CAM implementation for 3D models. \"\"\"\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output.detach()\n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0].detach()\n",
    "        self.hook_handles.append(self.target_layer.register_forward_hook(forward_hook))\n",
    "        self.hook_handles.append(self.target_layer.register_full_backward_hook(backward_hook))\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for handle in self.hook_handles:\n",
    "            handle.remove()\n",
    "\n",
    "    def __call__(self, input_tensor, target_class=None):\n",
    "        output = self.model(input_tensor)\n",
    "        if target_class is None:\n",
    "            target_class = output.argmax(dim=1)\n",
    "        one_hot = torch.zeros_like(output)\n",
    "        for i, tc in enumerate(target_class):\n",
    "            one_hot[i, tc] = 1\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        output.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "        gradients = self.gradients\n",
    "        activations = self.activations\n",
    "        weights = torch.mean(gradients, dim=(2, 3, 4), keepdim=True)\n",
    "        cam = torch.sum(weights * activations, dim=1)\n",
    "        cam = F.relu(cam)\n",
    "\n",
    "        cams = []\n",
    "        for i in range(cam.shape[0]):\n",
    "            cam_i = cam[i] - cam[i].min()\n",
    "            if cam_i.max() != 0:\n",
    "                cam_i /= cam_i.max()\n",
    "            cams.append(cam_i)\n",
    "        cams = torch.stack(cams)\n",
    "        target_size = input_tensor.shape[2:]\n",
    "        cams = F.interpolate(cams.unsqueeze(1), size=target_size, mode='trilinear', align_corners=False).squeeze(1)\n",
    "        return cams\n",
    "\n",
    "# =============================================================================\n",
    "# Model Definition\n",
    "# =============================================================================\n",
    "def load_model(config):\n",
    "    model = resnet18(spatial_dims=3, n_input_channels=1, num_classes=2)\n",
    "    state_dict = torch.load(config['pretrained_weights_path'], map_location=\"cpu\")\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    return model\n",
    "\n",
    "# =============================================================================\n",
    "# Utility Functions for Visualization\n",
    "# =============================================================================\n",
    "def get_sample_by_label(dataset, target_label):\n",
    "    for i in range(len(dataset)):\n",
    "        image, label = dataset[i]\n",
    "        if label == target_label:\n",
    "            return image, label\n",
    "    return None\n",
    "\n",
    "def generate_gradcam_plot(pdf, model, gradcam, sample, title):\n",
    "    device = next(model.parameters()).device\n",
    "    input_tensor = sample[0].unsqueeze(0).to(device)\n",
    "    cam = gradcam(input_tensor)[0].cpu().numpy()  # Shape: (D, H, W)\n",
    "    original = sample[0].squeeze(0).cpu().numpy()  # Shape: (D, H, W)\n",
    "\n",
    "    # Define slices used in dataset (17 to 32)\n",
    "    original_indices = np.arange(17, 33)  # The real slice numbers\n",
    "    num_slices = len(original_indices)\n",
    "\n",
    "    fig, axs = plt.subplots(num_slices, 2, figsize=(12, 2.5 * num_slices))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    for i, idx in enumerate(original_indices):\n",
    "        slice_idx = i  # Adjusted index for our dataset slices (0-based)\n",
    "        \n",
    "        # Original MRI Image\n",
    "        axs[i, 0].imshow(original[slice_idx], cmap='gray')\n",
    "        axs[i, 0].set_title(f\"Original Slice {idx}\", fontsize=10)\n",
    "        axs[i, 0].axis('off')\n",
    "\n",
    "        # CAM Overlay\n",
    "        axs[i, 1].imshow(original[slice_idx], cmap='gray')\n",
    "        axs[i, 1].imshow(cam[slice_idx], cmap='jet', alpha=0.5)\n",
    "        axs[i, 1].set_title(f\"Grad-CAM Overlay (Slice {idx})\", fontsize=10)\n",
    "        axs[i, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "# =============================================================================\n",
    "# Main Execution Function\n",
    "# =============================================================================\n",
    "def main():\n",
    "    config = {\n",
    "        'train_dataframe_path': 'Final_Datasets/train_resnet_heart.csv',\n",
    "        'test_dataframe_path': 'Final_Datasets/test_data_incidence.csv',\n",
    "        'pretrained_weights_path': '../Med3D/resnet_18_23dataset.pth',\n",
    "    }\n",
    "\n",
    "    # Load trained model\n",
    "    model = load_model(config)\n",
    "    model.eval()\n",
    "\n",
    "    # Load datasets\n",
    "    train_df = pd.read_csv(config['train_dataframe_path'])\n",
    "    test_df = pd.read_csv(config['test_dataframe_path'])\n",
    "    train_dataset = NPYDataset(train_df, 'FilePath', 'CAD')\n",
    "    test_dataset = NPYDataset(test_df, 'FilePath', 'CAD')\n",
    "\n",
    "    # Get case and control samples from train and test sets\n",
    "    train_case = get_sample_by_label(train_dataset, target_label=1)\n",
    "    train_control = get_sample_by_label(train_dataset, target_label=0)\n",
    "    test_case = get_sample_by_label(test_dataset, target_label=1)\n",
    "    test_control = get_sample_by_label(test_dataset, target_label=0)\n",
    "\n",
    "    # Use layer3 for Grad-CAM\n",
    "    target_layer = model.layer3[1].conv2\n",
    "    gradcam = GradCam3D(model, target_layer)\n",
    "\n",
    "    # Save all plots in a single PDF\n",
    "    pdf_filename = \"GradCAM_Results.pdf\"\n",
    "    with PdfPages(pdf_filename) as pdf:\n",
    "        if train_case:\n",
    "            generate_gradcam_plot(pdf, model, gradcam, train_case, \"Training Set: Case\")\n",
    "        if train_control:\n",
    "            generate_gradcam_plot(pdf, model, gradcam, train_control, \"Training Set: Control\")\n",
    "        if test_case:\n",
    "            generate_gradcam_plot(pdf, model, gradcam, test_case, \"Test Set: Case\")\n",
    "        if test_control:\n",
    "            generate_gradcam_plot(pdf, model, gradcam, test_control, \"Test Set: Control\")\n",
    "\n",
    "    gradcam.remove_hooks()\n",
    "    print(f\"Saved Grad-CAM results to {pdf_filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39347cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2615676/7854397.py:110: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(config['pretrained_weights_path'], map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Grad-CAM results for Slice 18 to GradCAM_Slice18_Comparison_ch0_Med3D.pdf\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from monai.networks.nets import resnet18  # MONAI’s 3D ResNet18\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# =============================================================================\n",
    "# Configure Logging\n",
    "# =============================================================================\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# =============================================================================\n",
    "# Data Loading: Custom Dataset for .npy Files\n",
    "# =============================================================================\n",
    "class NPYDataset(Dataset):\n",
    "    \"\"\" Custom PyTorch Dataset for loading 3D medical imaging data from .npy files. \"\"\"\n",
    "    def __init__(self, dataframe, image_column_name, label_column_name):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_column_name = image_column_name\n",
    "        self.label_column_name = label_column_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            npy_path = self.dataframe[self.image_column_name].iloc[idx]\n",
    "            label = self.dataframe[self.label_column_name].iloc[idx]\n",
    "            image = np.load(npy_path)[:, :, :, 0]  # Load single-channel data\n",
    "            image = image[17:33, :, :]  # Select slices 17 to 32\n",
    "            image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)  # Shape (1, D, H, W)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading image at index {idx}: {e}\")\n",
    "            raise\n",
    "\n",
    "# =============================================================================\n",
    "# Grad-CAM Implementation for 3D Data\n",
    "# =============================================================================\n",
    "class GradCam3D:\n",
    "    \"\"\" Grad-CAM implementation for 3D models. \"\"\"\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            if isinstance(output, tuple):  # Ensure output is a tensor\n",
    "                output = output[0]\n",
    "            self.activations = output.detach()\n",
    "\n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            if isinstance(grad_output, tuple):  # Ensure gradients are a tensor\n",
    "                grad_output = grad_output[0]\n",
    "            self.gradients = grad_output.detach()\n",
    "\n",
    "        self.hook_handles.append(self.target_layer.register_forward_hook(forward_hook))\n",
    "        self.hook_handles.append(self.target_layer.register_full_backward_hook(backward_hook))\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for handle in self.hook_handles:\n",
    "            handle.remove()\n",
    "\n",
    "    def __call__(self, input_tensor, target_class=None):\n",
    "        output = self.model(input_tensor)\n",
    "        if target_class is None:\n",
    "            target_class = output.argmax(dim=1)\n",
    "        one_hot = torch.zeros_like(output)\n",
    "        for i, tc in enumerate(target_class):\n",
    "            one_hot[i, tc] = 1\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        output.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "        gradients = self.gradients\n",
    "        activations = self.activations\n",
    "        weights = torch.mean(gradients, dim=(2, 3, 4), keepdim=True)\n",
    "        cam = torch.sum(weights * activations, dim=1)\n",
    "        cam = F.relu(cam)\n",
    "\n",
    "        cams = []\n",
    "        for i in range(cam.shape[0]):\n",
    "            cam_i = cam[i] - cam[i].min()\n",
    "            if cam_i.max() != 0:\n",
    "                cam_i /= cam_i.max()\n",
    "            cams.append(cam_i)\n",
    "        cams = torch.stack(cams)\n",
    "        target_size = input_tensor.shape[2:]\n",
    "        cams = F.interpolate(cams.unsqueeze(1), size=target_size, mode='trilinear', align_corners=False).squeeze(1)\n",
    "        return cams\n",
    "\n",
    "# =============================================================================\n",
    "# Model Definition\n",
    "# =============================================================================\n",
    "def load_model(config):\n",
    "    model = resnet18(spatial_dims=3, n_input_channels=1, num_classes=2)\n",
    "    state_dict = torch.load(config['pretrained_weights_path'], map_location=\"cpu\")\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    return model\n",
    "\n",
    "# =============================================================================\n",
    "# Utility Functions for Visualization\n",
    "# =============================================================================\n",
    "def get_sample_by_label(dataset, target_label):\n",
    "    for i in range(len(dataset)):\n",
    "        image, label = dataset[i]\n",
    "        if label == target_label:\n",
    "            return image, label\n",
    "    return None\n",
    "\n",
    "def generate_gradcam_plot(pdf, model, sample, layer_name, target_layer, sample_type):\n",
    "    device = next(model.parameters()).device\n",
    "    input_tensor = sample[0].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Compute Grad-CAM for the specified layer\n",
    "    gradcam = GradCam3D(model, target_layer)\n",
    "    cam = gradcam(input_tensor)[0].cpu().numpy()  # Shape: (D, H, W)\n",
    "    original = sample[0].squeeze(0).cpu().numpy()  # Shape: (D, H, W)\n",
    "    \n",
    "    # Slice 18 is index 1 in our tensor (since we select slices 17-32)\n",
    "    slice_idx = 1\n",
    "    slice_number = 18\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    fig.suptitle(f\"Grad-CAM for Slice {slice_number} ({layer_name}) - {sample_type}\", fontsize=16)\n",
    "\n",
    "    # Original MRI Image\n",
    "    axs[0].imshow(original[slice_idx], cmap='gray')\n",
    "    axs[0].set_title(f\"Original Slice {slice_number} - {sample_type}\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # CAM Overlay\n",
    "    axs[1].imshow(original[slice_idx], cmap='gray')\n",
    "    axs[1].imshow(cam[slice_idx], cmap='jet', alpha=0.5)\n",
    "    axs[1].set_title(f\"Grad-CAM Overlay (Layer {layer_name})\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "    gradcam.remove_hooks()  # Cleanup hooks after use\n",
    "\n",
    "# =============================================================================\n",
    "# Main Execution Function\n",
    "# =============================================================================\n",
    "def main():\n",
    "    config = {\n",
    "        'train_dataframe_path': 'Final_Datasets/train_resnet_heart.csv',\n",
    "        'pretrained_weights_path': '../Med3D/resnet_18_23dataset.pth',\n",
    "    }\n",
    "\n",
    "    # Load trained model\n",
    "    model = load_model(config)\n",
    "    model.eval()\n",
    "\n",
    "    # Load dataset\n",
    "    train_df = pd.read_csv(config['train_dataframe_path'])\n",
    "    train_dataset = NPYDataset(train_df, 'FilePath', 'CAD')\n",
    "\n",
    "    # Get one case and one control from the training set\n",
    "    train_case = get_sample_by_label(train_dataset, target_label=1)\n",
    "    train_control = get_sample_by_label(train_dataset, target_label=0)\n",
    "\n",
    "    # Define layers to analyze\n",
    "    layers_to_analyze = {\n",
    "        \"layer2\": model.layer2[1].conv2,\n",
    "        \"layer3\": model.layer3[1].conv2,\n",
    "        \"layer4\": model.layer4[1].conv2\n",
    "    }\n",
    "\n",
    "    # Save all plots in a single PDF\n",
    "    pdf_filename = \"GradCAM_Slice18_Comparison_ch0_Med3D.pdf\"\n",
    "    with PdfPages(pdf_filename) as pdf:\n",
    "        for layer_name, target_layer in layers_to_analyze.items():\n",
    "            if train_case:\n",
    "                generate_gradcam_plot(pdf, model, train_case, layer_name, target_layer, \"Case\")\n",
    "            if train_control:\n",
    "                generate_gradcam_plot(pdf, model, train_control, layer_name, target_layer, \"Control\")\n",
    "\n",
    "    print(f\"Saved Grad-CAM results for Slice 18 to {pdf_filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d29babe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_env",
   "language": "python",
   "name": "pyt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
