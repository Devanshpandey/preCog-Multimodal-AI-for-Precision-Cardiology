{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5597cd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from fastai.vision.all import *\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from fastai.learner import Learner\n",
    "from fastai.data.core import DataLoaders\n",
    "from fastai.metrics import accuracy\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.callback.all import SaveModelCallback, EarlyStoppingCallback\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "823d96b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load datasets\n",
    "# df = pd.read_csv('Final_Datasets/train_resnet_dxa.csv')\n",
    "# df_test = pd.read_csv('Final_Datasets/test_data_incidence.csv')\n",
    "# controls = pd.read_csv('Final_Datasets/imbalanced_control_iid.csv')\n",
    "\n",
    "# # Remove test cases and controls\n",
    "# test_iids = set(df_test['IID'])\n",
    "\n",
    "# # Get all cases (CAD = 1) directly from df, excluding test data\n",
    "# cases_train = df[(df['CAD'] == 1) & (~df['IID'].isin(test_iids))]\n",
    "\n",
    "# # Get all controls specifically from controls.csv, excluding test data\n",
    "# controls_train = controls[(~controls['IID'].isin(test_iids))]\n",
    "\n",
    "# # Combine the imbalanced dataset\n",
    "# df_imbalanced_train = pd.concat([cases_train, controls_train]).reset_index(drop=True)\n",
    "\n",
    "# # Save the imbalanced dataset\n",
    "# df_imbalanced_train.to_csv('Final_Datasets/train_imbalanced_dxa.csv', sep=',', index=False)\n",
    "\n",
    "# # Report the dataset statistics\n",
    "# cases_count = len(cases_train)\n",
    "# controls_count = len(controls_train)\n",
    "# print(f\"Number of cases: {cases_count}\")\n",
    "# print(f\"Number of controls: {controls_count}\")\n",
    "# print(f\"Imbalance ratio (controls to cases): {controls_count / cases_count:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1638572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backbone Class for Feature Extraction\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base_model = models.resnet50(pretrained=False)\n",
    "        encoder_layers = list(base_model.children())\n",
    "        self.backbone = nn.Sequential(*encoder_layers[:9])  # Use the first 9 layers of ResNet50\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "# Classifier Class (not used in embeddings extraction but included for completeness)\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.linear = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.drop_out(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# DXA Dataset Class\n",
    "class DXADataset(Dataset):\n",
    "    def __init__(self, dataframe, image_column_name, label_column_name, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_column_name = image_column_name\n",
    "        self.label_column_name = label_column_name\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe[self.image_column_name].iloc[idx]\n",
    "        label = self.dataframe[self.label_column_name].iloc[idx]\n",
    "\n",
    "        # Load the DICOM file\n",
    "        dicom = pydicom.dcmread(img_path)\n",
    "        image = dicom.pixel_array.astype(np.float32)\n",
    "\n",
    "        # Normalize the image\n",
    "        image -= np.min(image)\n",
    "        if np.max(image) != 0:\n",
    "            image /= np.max(image)\n",
    "\n",
    "        # Convert to PIL Image and ensure grayscale\n",
    "        image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "\n",
    "        # Convert grayscale to 3-channel\n",
    "        transform_to_3_channel = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3)\n",
    "        ])\n",
    "        image = transform_to_3_channel(image)\n",
    "\n",
    "        # Apply transformations if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label, img_path\n",
    "\n",
    "\n",
    "# DXA Embeddings Extraction Class\n",
    "class DXADiseaseModelEmbeddings:\n",
    "    def __init__(self, train_df_path, test_df_path, image_column_name, label_column_name, batch_size=32, model_name='dxa_radIM_resnet50_model_nov'):\n",
    "        self.train_df_path = train_df_path\n",
    "        self.test_df_path = test_df_path\n",
    "        self.image_column_name = image_column_name\n",
    "        self.label_column_name = label_column_name\n",
    "        self.batch_size = batch_size\n",
    "        self.model_name = model_name\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self._prepare_data()\n",
    "        self._prepare_model()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        train_df = pd.read_csv(self.train_df_path)\n",
    "        test_df = pd.read_csv(self.test_df_path)\n",
    "\n",
    "        # DXA Dataset\n",
    "        self.train_dataset = DXADataset(train_df, self.image_column_name, self.label_column_name, transform=self._get_transforms())\n",
    "        self.test_dataset = DXADataset(test_df, self.image_column_name, self.label_column_name, transform=self._get_transforms())\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=8)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "    def _get_transforms(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def _prepare_model(self):\n",
    "        backbone = Backbone()\n",
    "        classifier = Classifier(num_classes=2)\n",
    "        model = nn.Sequential(backbone, classifier)\n",
    "        model.to(self.device)\n",
    "        self.model = model\n",
    "\n",
    "        # Load the fine-tuned model\n",
    "        self.model.load_state_dict(torch.load(f'{self.model_name}.pth'))\n",
    "        self.model.eval()\n",
    "\n",
    "    def extract_embeddings(self, loader):\n",
    "        embeddings, labels, paths = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for images, label_batch, path_batch in loader:  # Add path_batch\n",
    "                images = images.to(self.device)\n",
    "            \n",
    "                # Pass through the backbone only\n",
    "                x = self.model[0](images)  # Extract features from the Backbone\n",
    "                x = torch.flatten(x, 1)   # Flatten after global pooling\n",
    "            \n",
    "                embeddings.append(x.cpu().numpy())\n",
    "                labels.append(label_batch.cpu().numpy())\n",
    "                paths.extend(path_batch)  # Collect the image paths\n",
    "        embeddings = np.concatenate(embeddings)\n",
    "        labels = np.concatenate(labels)\n",
    "        return embeddings, labels, paths\n",
    "\n",
    "    # Updated generate_embeddings_dataframe function\n",
    "    def generate_embeddings_dataframe(self, embeddings, labels, paths):\n",
    "        \"\"\"\n",
    "        Creates a Pandas DataFrame from embeddings, labels, and image paths.\n",
    "\n",
    "        Args:\n",
    "            embeddings (numpy.ndarray): The extracted embeddings.\n",
    "            labels (numpy.ndarray): The labels corresponding to the embeddings.\n",
    "            paths (list of str): The image paths.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame with serialized embeddings and metadata.\n",
    "        \"\"\"\n",
    "        # Serialize embeddings as JSON strings for safe CSV storage\n",
    "        df = pd.DataFrame({\n",
    "            'image_path': paths,\n",
    "            'embedding': [json.dumps(emb.tolist()) for emb in embeddings],\n",
    "            'label': labels\n",
    "        })\n",
    "        return df\n",
    "\n",
    "    # Updated extract_and_save_embeddings function\n",
    "    def extract_and_save_embeddings(self):\n",
    "        \"\"\"\n",
    "        Extracts embeddings for train and test datasets and saves them as CSV files.\n",
    "\n",
    "        The embeddings are serialized as JSON strings for robust CSV storage.\n",
    "        \"\"\"\n",
    "        # Extract training embeddings\n",
    "        train_embeddings, train_labels, train_paths = self.extract_embeddings(self.train_loader)\n",
    "        train_df = self.generate_embeddings_dataframe(train_embeddings, train_labels, train_paths)\n",
    "\n",
    "        # Extract test embeddings\n",
    "        test_embeddings, test_labels, test_paths = self.extract_embeddings(self.test_loader)\n",
    "        test_df = self.generate_embeddings_dataframe(test_embeddings, test_labels, test_paths)\n",
    "\n",
    "        # Save DataFrames\n",
    "        train_df.to_csv('train_embeddings_dxa_nov_imbalanced.csv', index=False)\n",
    "        test_df.to_csv('test_embeddings_dxa_nov.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ca64f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/07880/devansh/anaconda3/envs/pyt_env/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/work/07880/devansh/anaconda3/envs/pyt_env/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_6540/186359804.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(f'{self.model_name}.pth'))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = DXADiseaseModelEmbeddings(\n",
    "        train_df_path='Final_Datasets/train_imbalanced_dxa.csv',  # Path to DXA train dataset\n",
    "        test_df_path='Final_Datasets/test_data_incidence.csv',    # Path to DXA test dataset\n",
    "        image_column_name='FilePath_dxa',                        # DXA image file paths\n",
    "        label_column_name='CAD',                                 # CAD labels\n",
    "        model_name='models/dxa_radIM_resnet50_model_nov'         # Fine-tuned DXA model name\n",
    "    )\n",
    "    \n",
    "    # Extract embeddings and save to CSV\n",
    "    model.extract_and_save_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56e82e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_env",
   "language": "python",
   "name": "pyt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
